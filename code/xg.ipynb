{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc849962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated scale_pos_weight for XGBoost: 777.64\n",
      "\n",
      "--- XGBoost Classification Report (New Features) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    190634\n",
      "           1       0.22      0.96      0.36       245\n",
      "\n",
      "    accuracy                           1.00    190879\n",
      "   macro avg       0.61      0.98      0.68    190879\n",
      "weighted avg       1.00      1.00      1.00    190879\n",
      "\n",
      "\n",
      "--- XGBoost Confusion Matrix (New Features) ---\n",
      "[[189787    847]\n",
      " [     9    236]]\n"
     ]
    }
   ],
   "source": [
    "# MLOps Stage: Training the Robust Model\n",
    "# This code integrates all feature engineering, scaling, and XGBoost training.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --- I. Data Loading & Feature Engineering (The Robust Recipe) ---\n",
    "\n",
    "# Load the Sampled Data (Ensure this file is in the same directory as your notebook)\n",
    "df = pd.read_csv(\"E:\\FraudPulse\\Data\\AIML_Sample_10Pct.csv\") \n",
    "\n",
    "# 1. Calculate Core Engineered Features (Balance Differences)\n",
    "df[\"balanceDiffOrig\"] = df[\"oldbalanceOrg\"] - df[\"newbalanceOrig\"]\n",
    "df[\"balanceDiffDest\"] = df[\"newbalanceDest\"] - df[\"oldbalanceDest\"]\n",
    "\n",
    "# 2. Create the 'is_merchant' Behavioral Feature\n",
    "df[\"is_merchant\"] = df[\"nameDest\"].str.startswith('M').astype(int)\n",
    "\n",
    "# 3. Create the simplified velocity feature (Orig_Count_1step)\n",
    "# This feature captures the number of transactions by the same user in the same hour (step - 1)\n",
    "count_by_user_step = df.groupby(['nameOrig', 'step'])['amount'].count().reset_index()\n",
    "count_by_user_step.rename(columns={'amount': 'Orig_Count_1step_Total'}, inplace=True)\n",
    "\n",
    "df = df.merge(count_by_user_step, on=['nameOrig', 'step'], how='left')\n",
    "df['Orig_Count_1step'] = df['Orig_Count_1step_Total'] - 1\n",
    "\n",
    "# 4. Final Data Cleanup (Remove redundant/non-numeric columns)\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud', 'step', 'Orig_Count_1step_Total']) \n",
    "\n",
    "# --- II. Model Training Setup ---\n",
    "\n",
    "X = df.drop(\"isFraud\", axis=1)\n",
    "y = df[\"isFraud\"]\n",
    "\n",
    "# 5. Data Splitting (Stratify handles the severe class imbalance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6. Calculate Imbalance Weight (The key to fixing 0.02 Precision)\n",
    "fraud_count = y_train.value_counts()[1]\n",
    "non_fraud_count = y_train.value_counts()[0]\n",
    "scale_pos_weight = non_fraud_count / fraud_count\n",
    "print(f\"Calculated scale_pos_weight for XGBoost: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# 7. Define Preprocessing Pipeline (Imputation/Scaling/Encoding)\n",
    "numeric_features = [\n",
    "    \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\",\n",
    "    \"balanceDiffOrig\", \"balanceDiffDest\", \"is_merchant\", \"Orig_Count_1step\"\n",
    "]\n",
    "categorical_features = [\"type\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# 8. Build and Train the XGBoost Pipeline\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight, # Apply the imbalance weight\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5 \n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model (This step takes time)\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- III. Evaluation ---\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n--- XGBoost Classification Report (New Features) ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n--- XGBoost Confusion Matrix (New Features) ---\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0efbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline successfully saved to: E:\\FraudPulse\\models\\fraud_detection_deployment_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the full path, ensuring the directory exists\n",
    "model_path = r\"E:\\FraudPulse\\models\"\n",
    "file_name = \"fraud_detection_deployment_pipeline.pkl\"\n",
    "full_path = os.path.join(model_path, file_name)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Save the entire pipeline object to the specified location\n",
    "joblib.dump(xgb_pipeline, full_path)\n",
    "\n",
    "print(f\"✅ Pipeline successfully saved to: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06cf44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline loaded successfully for testing.\n",
      "\n",
      "--- Prediction Output ---\n",
      "Predicted Class: 0 (0=Safe, 1=Fraud)\n",
      "Confidence Score (Probability of Fraud): 0.0001\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the exact path where you saved the model\n",
    "MODEL_PATH = r\"E:\\FraudPulse\\models\\fraud_detection_deployment_pipeline.pkl\"\n",
    "\n",
    "# --- A. Load the Saved Pipeline ---\n",
    "# This loads the preprocessor (scaling/encoding/imputation) AND the trained XGBoost model.\n",
    "try:\n",
    "    loaded_pipeline = joblib.load(MODEL_PATH)\n",
    "    print(\"✅ Pipeline loaded successfully for testing.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Model file not found at {MODEL_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# --- B. Simulate a New Transaction (Raw Data Input) ---\n",
    "# A new transaction comes in from the bank's system.\n",
    "# Note: It MUST include all the raw features our pipeline expects.\n",
    "new_transaction_data = pd.DataFrame([{\n",
    "    \"type\": \"TRANSFER\",\n",
    "    \"amount\": 250000.00,\n",
    "    \"oldbalanceOrg\": 300000.00,\n",
    "    \"newbalanceOrig\": 50000.00,\n",
    "    \"oldbalanceDest\": 1000.00,\n",
    "    \"newbalanceDest\": 251000.00,\n",
    "    \"isFlaggedFraud\": 0, # Not used in prediction, but included in raw input\n",
    "    \"step\": 300,\n",
    "    \"nameOrig\": \"C_TEST_SENDER\",\n",
    "    \"nameDest\": \"C_TEST_RECEIVER\"\n",
    "}])\n",
    "\n",
    "# --- C. Feature Engineering on the New Transaction (In Real-Time) ---\n",
    "# Your real-time API must calculate these engineered features instantly.\n",
    "new_transaction_data[\"balanceDiffOrig\"] = new_transaction_data[\"oldbalanceOrg\"] - new_transaction_data[\"newbalanceOrig\"]\n",
    "new_transaction_data[\"balanceDiffDest\"] = new_transaction_data[\"newbalanceDest\"] - new_transaction_data[\"oldbalanceDest\"]\n",
    "new_transaction_data[\"is_merchant\"] = new_transaction_data[\"nameDest\"].str.startswith('M').astype(int)\n",
    "\n",
    "# Create the velocity feature (In a real API, this would require querying the database for past transactions)\n",
    "# For this test, we'll assume no recent activity (Count = 0).\n",
    "new_transaction_data['Orig_Count_1step'] = 0\n",
    "\n",
    "# Drop columns not used by the pipeline's ColumnTransformer\n",
    "X_new = new_transaction_data.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud'])\n",
    "X_new = X_new.drop(columns=['step']) # Drop raw time\n",
    "\n",
    "# --- D. Predict ---\n",
    "prediction = loaded_pipeline.predict(X_new)[0]\n",
    "risk_score = loaded_pipeline.predict_proba(X_new)[0][1]\n",
    "\n",
    "print(\"\\n--- Prediction Output ---\")\n",
    "print(f\"Predicted Class: {int(prediction)} (0=Safe, 1=Fraud)\")\n",
    "print(f\"Confidence Score (Probability of Fraud): {risk_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_pulse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
