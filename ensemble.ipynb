{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009dc0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble model components defined and configured with imbalance weights.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume scale_pos_weight is already calculated and available from Step 14\n",
    "# We will define the preprocessor again for completeness in the pipeline definition\n",
    "scale_pos_weight = 777.64 \n",
    "\n",
    "# --- 1. Define Preprocessing Pipeline (same as before) ---\n",
    "numeric_features = [\n",
    "    \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\",\n",
    "    \"balanceDiffOrig\", \"balanceDiffDest\", \"is_merchant\", \"Orig_Count_1step\"\n",
    "]\n",
    "categorical_features = [\"type\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# --- 2. Define Base Model Components ---\n",
    "\n",
    "# A. XGBoost (Gradient Boosting - Non-linear)\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "# B. Random Forest (Bagging Ensemble - Non-linear, lower variance)\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    # Use class_weight='balanced' equivalent to scale_pos_weight for RF\n",
    "    class_weight='balanced' \n",
    ")\n",
    "\n",
    "# C. Logistic Regression (Linear Model - Fast, gives a linear view)\n",
    "lr_clf = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    solver='saga',\n",
    "    max_iter=500 \n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ Ensemble model components defined and configured with imbalance weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d47dc7",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85920394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loading, feature engineering, and splitting complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Load the Sampled Data and Consolidate Feature Engineering ---\n",
    "\n",
    "# Load the Sampled Data (Ensure this file is in the same directory as your notebook)\n",
    "# Assuming 'AIML_Sample_10Pct.csv' is available\n",
    "try:\n",
    "    df = pd.read_csv(\"E:\\FraudPulse\\Data\\AIML_Sample_10Pct.csv\") \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: AIML_Sample_10Pct.csv not found. Please check file path.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Calculate Core Engineered Features (Balance Differences)\n",
    "df[\"balanceDiffOrig\"] = df[\"oldbalanceOrg\"] - df[\"newbalanceOrig\"]\n",
    "df[\"balanceDiffDest\"] = df[\"newbalanceDest\"] - df[\"oldbalanceDest\"]\n",
    "\n",
    "# 3. Create the 'is_merchant' Behavioral Feature\n",
    "df[\"is_merchant\"] = df[\"nameDest\"].str.startswith('M').astype(int)\n",
    "\n",
    "# 4. Create the simplified velocity feature (Orig_Count_1step)\n",
    "# This requires step and nameOrig\n",
    "count_by_user_step = df.groupby(['nameOrig', 'step'])['amount'].count().reset_index()\n",
    "count_by_user_step.rename(columns={'amount': 'Orig_Count_1step_Total'}, inplace=True)\n",
    "df = df.merge(count_by_user_step, on=['nameOrig', 'step'], how='left')\n",
    "df['Orig_Count_1step'] = df['Orig_Count_1step_Total'] - 1\n",
    "\n",
    "# 5. Final Data Cleanup\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'isFlaggedFraud', 'step', 'Orig_Count_1step_Total']) \n",
    "\n",
    "# --- 6. Define Data for Training ---\n",
    "X = df.drop(\"isFraud\", axis=1)\n",
    "y = df[\"isFraud\"]\n",
    "\n",
    "# 7. Define Data Splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "print(\"✅ Data loading, feature engineering, and splitting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fd762",
   "metadata": {},
   "source": [
    "### Execute Randomized Search\n",
    "\n",
    "need to run the final code block that executes the Randomized Search using the parameters and scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0021144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessor and XGBoost Pipeline objects defined.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Scale Weight (using the value you calculated) ---\n",
    "scale_pos_weight = 777.64 \n",
    "\n",
    "# --- 2. Define Preprocessing Pipeline (preprocessor) ---\n",
    "numeric_features = [\n",
    "    \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\",\n",
    "    \"balanceDiffOrig\", \"balanceDiffDest\", \"is_merchant\", \"Orig_Count_1step\"\n",
    "]\n",
    "categorical_features = [\"type\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# --- 3. Define the XGBoost Pipeline (xgb_pipeline) ---\n",
    "# This is the object required for RandomizedSearchCV\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight, # Apply the imbalance weight\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5 \n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"✅ Preprocessor and XGBoost Pipeline objects defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8470fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parameter grid (param_dist) and scoring metric (scorer) defined.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "# --- 1. Define the parameters to search (Parameter Grid) ---\n",
    "param_dist = {\n",
    "    # Search for optimal number of trees (100 to 500)\n",
    "    'classifier__n_estimators': randint(100, 500), \n",
    "    # Search for optimal learning rate (0.01 to 0.2)\n",
    "    'classifier__learning_rate': uniform(0.01, 0.19),\n",
    "    # Search for optimal tree depth (3 to 8)\n",
    "    'classifier__max_depth': randint(3, 8),\n",
    "    # Search for optimal regularization parameters\n",
    "    'classifier__colsample_bytree': uniform(0.6, 0.4) \n",
    "}\n",
    "\n",
    "# --- 2. Define the scoring metric (Crucial for Imbalanced Data) ---\n",
    "# We choose Recall as the primary metric to ensure we maximize fraud capture.\n",
    "scorer = make_scorer(recall_score)\n",
    "\n",
    "print(\"✅ Parameter grid (param_dist) and scoring metric (scorer) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703fe7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Randomized Search (Tuning XGBoost) ---\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "--- Best Parameters Found ---\n",
      "{'classifier__colsample_bytree': np.float64(0.8832290311184181), 'classifier__learning_rate': np.float64(0.013911053916202464), 'classifier__max_depth': 4, 'classifier__n_estimators': 443}\n"
     ]
    }
   ],
   "source": [
    "# Assuming the necessary imports (RandomizedSearchCV, etc.) are available from previous cells.\n",
    "\n",
    "# 3. Setup RandomizedSearchCV\n",
    "# n_iter=10 means we test 10 random combinations \n",
    "# cv=3 means we use 3-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_pipeline, # Use the pipeline with the XGBoost classifier\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring=scorer,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "# 4. Run the search (This step requires significant processing time)\n",
    "print(\"\\n--- Starting Randomized Search (Tuning XGBoost) ---\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. Extract Best Parameters ---\n",
    "print(\"\\n--- Best Parameters Found ---\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a88561",
   "metadata": {},
   "source": [
    "### Define Final Optimized Ensemble Models\n",
    "\n",
    "We redefine the models using the best parameters found and then define the Stacking Classifier pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9835f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Ensemble Pipeline (Stacking) defined.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "# Scale weight is needed for all base models\n",
    "scale_pos_weight = 777.64\n",
    "\n",
    "# --- 1. Define Optimized Base Models ---\n",
    "\n",
    "# A. XGBoost (Optimized)\n",
    "xgb_optimized = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    colsample_bytree=0.883,\n",
    "    learning_rate=0.014,\n",
    "    max_depth=4,\n",
    "    n_estimators=443\n",
    ")\n",
    "\n",
    "# B. Random Forest (RF - Non-linear view)\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=100, # Using default for speed\n",
    "    max_depth=5\n",
    ")\n",
    "\n",
    "# C. Logistic Regression (LR - Linear view)\n",
    "lr_clf = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    solver='saga',\n",
    "    max_iter=500 \n",
    ")\n",
    "\n",
    "# Define the base models for stacking\n",
    "estimators = [\n",
    "    ('xgb', xgb_optimized),\n",
    "    ('rf', rf_clf),\n",
    "    ('lr', lr_clf)\n",
    "]\n",
    "\n",
    "# --- 2. Define Final Stacking Pipeline ---\n",
    "# Use Logistic Regression as the Meta-Model to combine the outputs\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=estimators, \n",
    "    final_estimator=LogisticRegression(solver='saga', max_iter=500),\n",
    "    cv=3, # Use 3-fold cross-validation in the stacking process\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# The full pipeline containing the preprocessor and the Stacking Classifier\n",
    "final_ensemble_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', stack_clf)\n",
    "])\n",
    "\n",
    "print(\"✅ Final Ensemble Pipeline (Stacking) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9511f",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Ensemble\n",
    "\n",
    "* run the following code block, which executes the training and generates the final performance metrics for your robust stacking model.\n",
    "\n",
    "* This step involves training three powerful base models (XGBoost, Random Forest, Logistic Regression) and a meta-model, which will take longer than the single XGBoost model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e0f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Ensemble Training ---\n",
      "\n",
      "--- Stacking Ensemble Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    190634\n",
      "           1       0.89      0.80      0.84       245\n",
      "\n",
      "    accuracy                           1.00    190879\n",
      "   macro avg       0.95      0.90      0.92    190879\n",
      "weighted avg       1.00      1.00      1.00    190879\n",
      "\n",
      "\n",
      "--- Stacking Ensemble Confusion Matrix ---\n",
      "[[190611     23]\n",
      " [    50    195]]\n"
     ]
    }
   ],
   "source": [
    "# Train the Stacking Ensemble Model (This will take longer than the single XGBoost model)\n",
    "print(\"\\n--- Starting Final Ensemble Training ---\")\n",
    "final_ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "y_pred_ensemble = final_ensemble_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Stacking Ensemble Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_ensemble))\n",
    "\n",
    "print(\"\\n--- Stacking Ensemble Confusion Matrix ---\")\n",
    "print(confusion_matrix(y_test, y_pred_ensemble))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_pulse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
